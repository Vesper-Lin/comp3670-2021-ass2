{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP3670/6670 Programming Assignment 2 - Clustering, Linear Regression and Gradient Descent\n",
    "---\n",
    "\n",
    "**Enter Your Student ID:** u6828533\n",
    "\n",
    "**Your Name:** Yuxuan Lin\n",
    "    \n",
    "**Deadline:** 23:59pm, 19 September, 2021\n",
    "\n",
    "**Submit:** Write your answers in this file, and submit a single Jupyter Notebook file (.ipynb) on Wattle. Rename this file with your student number as 'u6828533.ipynb'. Note: you don't need to submit the .png or .npy files. \n",
    "\n",
    "**Enter Discussion Partner IDs Below:**\n",
    "You could add more IDs with the same markdown format above.\n",
    "\n",
    "**Programming Section**:\n",
    "- 1: 30%\n",
    "- 2: 40%\n",
    "- 3: 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Clustering\n",
    "-----------\n",
    "These programming exercises will focus on K-means clustering. \n",
    "\n",
    "If you're unsure of how k-means works, read this very helpful and freely available online breakdown from Stanford's CS221 course; https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "\n",
    "This assignment requires you to loosely interpret how k-means is a specific case of a more general algorithm named Expectation Maximisation. This is explained toward the end of the above article.\n",
    "\n",
    "First, lets loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.load(\"./data_clustering.npy\")\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is a special, simple case of the Expectation Maximisation (EM) algorithm.\n",
    "\n",
    "This simplified EM (k-means), is divided into two steps.\n",
    "\n",
    "The **E-Step**, where for every sample in your dataset you find which \"centroid\" that datapoint is closest to that sample, and record that information.\n",
    "\n",
    "The **M-Step**, where you move each \"centroid\" to the center of the samples which were found to be closest to it in the **E-Step**.\n",
    "\n",
    "Each *centroid* is simply an estimated mean of a cluster. If you have $1$ centroid, then this centroid will become the mean of all your data.\n",
    "\n",
    "Centroids are initially random values, and the k-means algorithm attempts to modify them so that each one represents the center of a cluster.\n",
    "\n",
    "We have implemented a centroids initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55638768  1.19083041]\n",
      " [ 0.99468733 -0.63105385]\n",
      " [-0.80861347 -0.47487527]\n",
      " [ 0.83443335  0.7038998 ]]\n"
     ]
    }
   ],
   "source": [
    "def initialise_parameters(m, X):\n",
    "    C = X[np.random.choice(X.shape[0], m)]\n",
    "    return C\n",
    "\n",
    "C = initialise_parameters(4, X)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement K-Means algorithm.\n",
    "\n",
    "---\n",
    "   **TASK 1.1:** Create a function $E\\_step(C, X) = L$, where $L$ is a matrix of the same dimension of the dataset $X$.\n",
    "   \n",
    "   This function is is the **E-Step** (or \"assignment step\") mentioned earlier.\n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** \n",
    "- https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "- https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm\n",
    "- Each row of $L$ is a centroid taken from $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-0d662f4661ac>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-0d662f4661ac>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    L = E_step(C, X)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def E_step(C, X):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "L = E_step(C, X)\n",
    "plt.scatter(L[:, 0], L[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1.2:** Create a function $M\\_step(C, X, L) = C$ which returns $C$ modified so that each centroid in $C$ is placed in the middle of the samples assigned to it. This is the **M-Step**.\n",
    "\n",
    "In other words, make each centroid in $C$ the average of all the samples which were found to be closest to it during the **E-step**. This is also called the \"update step\" for K-means.\n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_equal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(C, X, L):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "print('Before:')\n",
    "print(C)\n",
    "print('\\nAfter:')\n",
    "new_C = M_step(C, X, L)\n",
    "print(new_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1.3:** Implement $kmeans(X, m, i) = C, L$ which takes a dataset $X$ (of any dimension) and a scalar value $m$, and uses the previous 3 functions you wrote to:\n",
    "- generate $m$ centroids.\n",
    "- iterate between the E and M steps $i$ times (ie, it iterates $i$ times) to classify the $m$ clusters.\n",
    "\n",
    "...and then returns:\n",
    "- $C$, the centers of the $m$ clusters after $i$ iterations.\n",
    "- $L$, the labels (centroid values) assigned to each sample in the dataset after $i$ iterations.\n",
    "---\n",
    "**HINT:** Using initialise_parameters to initial centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, m, i):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "#CODE TO DISPLAY YOUR RESULTS. DO NOT MODIFY.\n",
    "C_final, L_final = kmeans(X, 4, 10)\n",
    "print('Initial Parameters:')\n",
    "print(C)\n",
    "print('\\nFinal Parameters:')\n",
    "print(C_final)\n",
    "\n",
    "def allocator(X, L, c):\n",
    "    cluster = []\n",
    "    for i in range(L.shape[0]):\n",
    "        if np.array_equal(L[i, :], c):\n",
    "            cluster.append(X[i, :])\n",
    "    return np.asarray(cluster)\n",
    "\n",
    "colours = ['r', 'g', 'b', 'y']\n",
    "for i in range(4):\n",
    "    cluster = allocator(X, L_final, C_final[i, :])\n",
    "    plt.scatter(cluster[:,0], cluster[:,1], c=colours[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer should like this, maybe with different colors:\n",
    "![image](./cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TASK 1.4:** Explain how do you find the number of centroids\n",
    "\n",
    "---\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Linear Regression and Gradient Descent\n",
    "---\n",
    "\n",
    "For exercise 2, we're going to implement multiple target **batch** linear regression with mean squared loss,\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2 m} \\sum_{i = 0}^{m} \\mid \\mid x_i\\theta - y_i \\mid \\mid^2$$.\n",
    "\n",
    "For the following questions:\n",
    "- $x \\in \\mathbb{R}^{m}$ is the vector directly representing input features from the provided dataset. Each row of $x$ is a single training example.\n",
    "- $X \\in \\mathbb{R}^{m \\times n}$ is the constructed feature matrix (e.g. polynomial features) used for learning. Each row of $X$ is a single training example.\n",
    "- $\\theta$ is our parameters. In the linear regression you've seen thus far, this is a vector. However, as we're doing multiple target linear regression, $\\theta$ will be a matrix.\n",
    "- $y \\in \\mathbb{R}^{m}$ is a matrix of the target values we're trying to estimate for each row of $X$. Each row $i$ of $X$ corresponds to row $i$ of $Y$.\n",
    "- $m$ is the number of training examples.\n",
    "- $n$ is the dimensionality of one training example.\n",
    "\n",
    "Typically when people think of linear regression, they think of a mapping from $\\mathbb{R}^n \\rightarrow \\mathbb{R}$, where they're trying to predict a single scalar value.\n",
    "\n",
    "---\n",
    "First, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = np.load(\"./data_regression.npy\")\n",
    "plt.plot(x_train,y_train,'o') ## YOUR CODE HERE\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(\"Training data\")\n",
    "plt.ylim([-1,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that it is not a good idea to perform linear regression directly on the input feature `x`. We need to add polynomial features. Lets construct an appropriate feature vector.\n",
    "\n",
    "---\n",
    "**Task 2.1**:  Complete the `get_polynomial_features` function with the following specifications.\n",
    "* Input1: an array `x` of shape $(m,1)$.\n",
    "* Input2: `degree` of the polynomial (integer greater than or equal to one).\n",
    "* Output: matrix of shape $(m,degree+1)$ consisting of horizontally concatenated polynomial terms.\n",
    "* Output: the first column of output matrix should be all ones.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_features(x,degree=5):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "# get polynomial features\n",
    "X_train = get_polynomial_features(x_train,degree=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement gradient descent to find the optimal $\\theta$.\n",
    "\n",
    "\n",
    "---\n",
    "**TASK 2.2:** Write a function $initialise\\_parameters(n) = \\theta$, where $\\theta$ is the parameters we will use for linear regression $X\\theta = Y$ for $X \\in \\mathbb{R}^{m \\times n}, Y \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "The values of $\\theta$ should be randomly generated. You will be judged on whether the matrix $\\theta$ is correctly constructed for this problem.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** $\\theta$ should be an array of length $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialise_parameters(n):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# initialize theta\n",
    "theta = initialise_parameters(X_train.shape[1])\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TASK 2.3:** Implement a function $ms\\_error(X, \\theta, y) = err$, which gives the **mean** squared error over all $m$ training examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_error(X, theta, y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(ms_error(X_train, theta, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TASK 2.4:** Implement $grad(X, \\theta, Y) = g$, a function that returns the average gradient ($\\partial \\mathcal{L}/\\partial {\\theta}$) across all the training examples $x_i \\in \\mathbb{R}^{1 \\times n}$.\n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** \n",
    "- The gradient should be an array with same length as $\\theta$.\n",
    "- https://www.sharpsightlabs.com/blog/numpy-sum/\n",
    "- https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html\n",
    "- https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, theta, Y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(grad(X_train, theta, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TASK 2.5:** Implement $batch\\_descent(X, Y, iterations, learning\\_rate) = \\theta, L$, a function which implements batch gradient descent returning $\\theta$ (parameters which estimate $Y$ from $X$), and $L$.\n",
    "\n",
    "$iterations$ is the number of gradient descent iterations to be performed.\n",
    "\n",
    "$learning\\_rate$ is, of course, the learning rate.\n",
    "\n",
    "$L$ is a matrix recording the mean squared error at every iteration of gradient descent. It will be an array of length $iterations$.\n",
    "\n",
    "You should use the functions you completed earlier to complete this. \n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** \n",
    "- Remember, the point of gradient descent is to minimise the loss function. \n",
    "- It does this by taking \"steps\". The gradient always points in the steepest direction uphill, so by stepping in the opposite direction of the gradient we move toward the value of $\\theta$ that minimises the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_descent(X, Y, iterations, learning_rate):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "#REPORTING CODE. YOU MAY NEED TO MODIFY THE LEARNING RATE OR NUMBER OF ITERATIONS\n",
    "new_theta, L = batch_descent(X_train, y_train, 5000, 0.5)\n",
    "plt.plot(L)\n",
    "plt.title('Mean Squared Error vs Iterations')\n",
    "plt.show()\n",
    "print('New Theta: \\n', new_theta)\n",
    "print('\\nFinal Mean Squared Error: \\n', ms_error(X_train, new_theta, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Regularization and Model Selection\n",
    "---\n",
    "\n",
    "In task 2, we focussed on using gradient descent to do linear regression with a polynomial of degree 5.\n",
    "\n",
    "Next, we would try to select a model that gives best performance on the val set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 3.1**:  Visualize the prediction curves for different choice of degree polynomial features, by completing the code below. \n",
    "\n",
    "- You can use the closed form solution or gradient descent for computing $\\theta$.\n",
    "- Compute the predictions on val data using `x_val` and computed $\\theta$.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta(X,y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "def get_prediction(X,theta):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "for degree in range(1,10):\n",
    "    # prepare train/val data\n",
    "    X_train = get_polynomial_features(x_train,degree=degree)\n",
    "    x_val = np.linspace(-0.7, 0.8, x_val.shape[0])\n",
    "    X_val = get_polynomial_features(x_val,degree=degree)\n",
    "    \n",
    "    # get theta\n",
    "    theta = get_theta(X_train,y_train)\n",
    "    \n",
    "    # compute predictions on train/val set\n",
    "    pred_y_train = get_prediction(X_train,theta)\n",
    "    pred_y_val = get_prediction(X_val,theta)\n",
    "    \n",
    "    # plot results\n",
    "    plt.plot(x_train,y_train,'o',label='train')\n",
    "    plt.plot(x_val,pred_y_val,label='val')\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(\"polynomial degree: {}\".format(degree))\n",
    "    plt.ylim([-1,3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 2.2**:  Draw the train, val loss curve for different degree polynomials by completing the following code.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = np.load(\"./data_regression.npy\")\n",
    "# store train/val loss values\n",
    "train_loss,val_loss = [],[]\n",
    "\n",
    "for degree in range(1,10):\n",
    "    # prepare train/val data\n",
    "    X_train =     # YOUR CODE HERE\n",
    "    X_val =     # YOUR CODE HERE\n",
    "\n",
    "    # get theta\n",
    "    theta = get_theta(# YOUR CODE HERE)\n",
    "    \n",
    "    # compute train/val losses\n",
    "    train_loss.append(# YOUR CODE HERE)\n",
    "    val_loss.append(# YOUR CODE HERE)\n",
    "    \n",
    "\n",
    "plt.plot(range(1,10),train_loss,'-o',label='train loss')\n",
    "plt.plot(range(1,10),val_loss,'-o',label='val loss')\n",
    "plt.xticks(range(1,10))\n",
    "plt.legend()\n",
    "plt.title('Train/Val Loss vs polynomial order')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 2.3**:  What is the best choice for degree of polynomial features suitable for this problem?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer**: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
